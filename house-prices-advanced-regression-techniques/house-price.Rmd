---
title: "Final Project Kaggle Competition"
author: "Shusei Yokoi"
date: "6/3/2021"
output: html_document
---

```{r setup, include=FALSE}
setwd('/Users/shuseiyokoi/Desktop/shutats/kaggle/house-prices-advanced-regression-techniques')
library(tidyverse)
library(tidymodels)
library(lattice)
library(gcookbook)
```

```{r data, include=FALSE}
set.seed(1)
house = read_csv('train.csv')
colnames(house)[44:45] <- c('X1stFlrSF', 'X2ndFlrSF')

house = house %>%
  dplyr::select(-Alley, -PoolQC, -Fence, -MiscFeature) %>% 
  drop_na() 
# 
# house_n = dplyr::select_if(house, is.numeric)
# house_n[is.na(house_n)] = 0
# 
# house_c = dplyr::select_if(house, is.character)
# 
# col_name = colnames(house_c)
# 
# house_c_modi = 0
# for( x in house_c[1:length(house_c)]){
#   if(anyNA(x)){
#     a = names(sort(table(x), decreasing = TRUE))[1]
#     house_c_modi = data.frame(house_c_modi, a)
#   }else{
#     house_c_modi = data.frame(house_c_modi,x )
#   }
# }
# house_c_modi = house_c_modi[-1]
# colnames(house_c_modi) = col_name
# 
# house = cbind(house_n, house_c_modi)

fn_test = read_csv('test.csv')
colnames(fn_test)[44:45] <- c('X1stFlrSF', 'X2ndFlrSF')

fn_test = fn_test%>% 
  dplyr::select(-Alley, -PoolQC, -Fence, -MiscFeature, -BsmtExposure)
# 
# fn_test_n = dplyr::select_if(fn_test, is.numeric)
# fn_test_n[is.na(fn_test_n)] = 0
# 
# fn_test_c = dplyr::select_if(fn_test, is.character)
# col_name = colnames(fn_test_c)
# 
# fn_test_c_modi = 0
# for( x in fn_test_c[1:length(fn_test_c)]){
#   if(anyNA(x)){
#     a = names(sort(table(x), decreasing = TRUE))[1]
#     fn_test_c_modi = data.frame(fn_test_c_modi, a)
#   }else{
#     fn_test_c_modi = data.frame(fn_test_c_modi,x )
#   }
# }
# fn_test_c_modi = fn_test_c_modi[-1]
# colnames(fn_test_c_modi) = col_name

# fn_test = cbind(fn_test_n, fn_test_c_modi)
```
```{r,include=FALSE}
train = house %>% 
  initial_split() %>% 
  training()
test = house %>% 
  initial_split() %>% 
  testing()
id = fn_test[1]
```

# Overview 
| Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.
| 
| With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. 

| Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)

With 79 explanatory variables, there are many missing data. Such as Alley, PoolQC, Fence, and MiscFeature contains 90% NA value. If the majority of explanatory variables are NA values, I simply removed from the training data set. Other than that, I simply replaced them with the most common class; for categorical and a median; for numeric variable. 
In the test data set given, there are a lot of NA values and new categories that does not appears in training data set. Therefore, Most of the explanatory variables are not helpful. 


# Variable selection (Numeric) 

I pick variables with correlation of more than |0.3|. 
```{r}
house_num<- dplyr::select_if(house, is.numeric)

r <- cor(house_num, use="complete.obs") %>% 
  data.frame()

ggcorrplot::ggcorrplot(r, hc.order = TRUE, type = "lower")

variable = colnames(r[r$SalePrice > abs(0.3)])
variable = head(variable, -1)
```
Here are the numeric variables that was +|0.3| correlation to Sales Price `r variable`   

```{r}
pairs(train[c(variable, "SalePrice")], pch = 16)
```

# Model Fit 
I fitted valiables that are significant to the sale price (OverallQual + YearBuilt + YearRemodAdd + MasVnrArea + TotalBsmtSF + BsmtFinSF1 + X1stFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + GarageYrBlt + GarageCars + GarageArea + factor(LandContour) + factor(BldgType) + factor(BsmtQual)  + factor(BsmtFinType1)  + factor(CentralAir)  + factor(KitchenQual)).

```{r}
set.seed(1)
lr_mod <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

lm_2 <- lr_mod %>%
  fit(log(SalePrice) ~ OverallQual + YearBuilt + YearRemodAdd + MasVnrArea + TotalBsmtSF + BsmtFinSF1 + X1stFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + GarageYrBlt + GarageCars + GarageArea + factor(LandContour) + factor(BldgType) + factor(BsmtQual)  + factor(BsmtFinType1)  + factor(CentralAir)  + factor(KitchenQual), data = train) 
summary(lm_2$fit)
```

RMSE for the linear regression is below. 
```{r}
prediction = exp(predict(lm_2, test))
prediction[is.na(prediction)] = median(prediction$.pred, na.rm = T)
rmse(test, SalePrice, prediction$.pred)
```

```{r, warning=F, include=FALSE}
prediction = exp(predict(lm_2, fn_test))
prediction[is.na(prediction)] = median(prediction$.pred, na.rm = T)
submission_1 = data.frame(id, prediction)
colnames(submission_1) = c('Id','SalePrice')
write.csv(submission_1, 'pred_ver1.csv',row.names = FALSE)
```


# LASSO Ridge Regression 

Since there are too many new/NA variables on the test set, I dropped about 20 categorical exploratory variables. Then I fitted LASSO and Ridge regression. However, it does not give me a good result. 
```{r, include=F}
# lapply(house_cha, table)
# fn_test_cha <- dplyr::select_if(fn_test, is.character)
# lapply(fn_test_cha, table)
```

```{r, include=FALSE}
house = house %>%
  select(-MSZoning, -Heating, -SaleType, -RoofStyle, -Exterior1st, -Exterior2nd, -SaleCondition, -GarageType, -Functional, -GarageCond, -GarageQual, -Electrical, -ExterCond, -RoofMatl, -LandSlope, -Utilities, -Foundation, -BsmtQual, -BsmtCond, -BsmtFinType1, -BsmtFinType2, -HouseStyle, -MasVnrType, -BsmtExposure, -KitchenQual, - FireplaceQu, -GarageFinish, -Id, -Street) %>% 
  fastDummies::dummy_cols(remove_selected_columns = T) %>% 
  na.omit()

fn_test = fn_test %>% 
    select(-MSZoning, -Heating, -SaleType, -SaleCondition, -RoofStyle, -Exterior1st, - Exterior2nd, -GarageType, -Functional, -GarageCond, -GarageQual, -Electrical, -ExterCond, -RoofMatl, -LandSlope, -Utilities, -Foundation, -BsmtQual, -BsmtCond, -BsmtFinType1, -BsmtFinType2, -HouseStyle, -MasVnrType, -KitchenQual, -FireplaceQu, -GarageFinish, -Id, -Street) %>% 
  fastDummies::dummy_cols(remove_selected_columns = T) 
```


```{r, include=FALSE}
set.seed(1)
ini_sp = initial_split(house)
train = ini_sp %>% 
  training()
test = ini_sp%>% 
  testing()
```

```{r}
set.seed(1)
mix_grid <- grid_regular(penalty(c(0, 1), trans = NULL), 
                         mixture(c(0, 1)), levels = 10)


mix_rec <- recipe(SalePrice ~ ., data = train) %>%
  step_naomit(all_numeric(), skip = T) %>%
  step_naomit(all_nominal(), skip = T) %>%
  step_dummy(all_nominal()) 

mix_spec_tune <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

mix_wflow <- workflow() %>% 
  add_recipe(mix_rec) %>% 
  add_model(mix_spec_tune)

# k-folds cross validation
train_cv <- vfold_cv(train, v = 5)

mix_grid_search <-
  tune_grid(
    mix_wflow,
    resamples = train_cv,
    grid = mix_grid
  )

tuning_grid <- mix_grid_search %>% 
  collect_metrics() %>% 
  filter(.metric == 'rmse') %>% 
  data.frame()
  
  
tuning_grid %>% 
  arrange(mean)

autoplot(mix_grid_search, metric = 'rmse')
```

The tune grid function suggested me to ued the ridge regression (mixture = 0) and panlty = 0. The estimate for variables are below. 



```{r, echo=F}
mix_spec <- linear_reg(penalty = 0, mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("regression") 

mix_wflow <- workflow() %>% 
  add_recipe(mix_rec) %>% 
  add_model(mix_spec)

mix_fit <- mix_wflow %>% 
  fit(train) 

mix_fit %>% 
  pull_workflow_fit() %>% 
  tidy() %>% 
  arrange(desc(estimate))
```

The RMSE for the Ridge regression is below. 
```{r, echo=F}
prediction = predict(mix_fit, test)
prediction[is.na(prediction)] = median(prediction$.pred, na.rm = T)
rmse(test, SalePrice, prediction$.pred)
```


```{r, include=FALSE}
prediction = predict(mix_fit, fn_test)
prediction[is.na(prediction)] = median(prediction$.pred, na.rm = T)
submission_2 = data.frame(id, prediction)
colnames(submission_2) = c('Id','SalePrice')
write.csv(submission_2, 'pred_ver2.csv', row.names = FALSE)
```




# k-NN 
I runed k-NN model to the data. The variables that are significant to the sale price (OverallQual + YearBuilt + YearRemodAdd + MasVnrArea + TotalBsmtSF + BsmtFinSF1 + X1stFlrSF + GrLivArea + FullBath+ TotRmsAbvGrd + GarageYrBlt + GarageCars + GarageArea) 

```{r , include=FALSE}
set.seed(1)
house = read_csv('train.csv')
colnames(house)[44:45] <- c('X1stFlrSF', 'X2ndFlrSF')

house = house %>%
  dplyr::select(-Alley, -PoolQC, -Fence, -MiscFeature) %>% 
  drop_na() 

fn_test = read_csv('test.csv')
colnames(fn_test)[44:45] <- c('X1stFlrSF', 'X2ndFlrSF')

fn_test = fn_test%>% 
  dplyr::select(-Alley, -PoolQC, -Fence, -MiscFeature, -BsmtExposure)

```
```{r,include=FALSE}
set.seed(1)
train = house %>% 
  initial_split() %>% 
  training()
test = house %>% 
  initial_split() %>% 
  testing()
id = fn_test[1]
```


```{r}
set.seed(1)
knn_mod_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")

k_grid <- grid_regular(neighbors(c(1,50)),  levels = 25)

knn_rec <- recipe(SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + MasVnrArea + TotalBsmtSF + BsmtFinSF1 + X1stFlrSF + GrLivArea + FullBath+ TotRmsAbvGrd + GarageYrBlt + GarageCars + GarageArea, data = train) %>%
  step_naomit(all_numeric(), skip = T) %>% 
  step_naomit(all_nominal(), skip = T) %>% 
  step_dummy(all_nominal()) %>% 
  step_normalize(all_numeric(), -SalePrice)

knn_wflow <- workflow() %>%
  add_recipe(knn_rec) %>%
  add_model(knn_mod_tune)

train_cv <- vfold_cv(train, v = 5)

knn_grid_search <-
  tune_grid(
    knn_wflow,
    resamples = train_cv,
    grid = k_grid
  )
```

The tuning grid suggested me to use 13 neighbors (k = 13) with mean RMSE around 40000. 
```{r, echo=F}
k = knn_grid_search %>% 
  collect_metrics() %>% 
  filter(.metric == 'rmse') %>% 
  slice_min(mean)
k
```

The RMSE for the Ridge regression is below. 
```{r}
knn_mod <- nearest_neighbor(neighbors = k$neighbors) %>%
  set_engine("kknn") %>%
  set_mode("regression")

ins_wflow <- workflow() %>%
  add_recipe(knn_rec) %>%
  add_model(knn_mod)

knn_fit <- ins_wflow %>% fit(test) 

p = predict(knn_fit, test)
rmse(test, SalePrice, p$.pred)
```

```{r , include=F}
# knn_fit <- ins_wflow %>% fit(train) 
# prediction = predict(knn_fit, fn_test)
# prediction[is.na(prediction)] = median(prediction$.pred, na.rm = T)
# submission_1 = data.frame(id, prediction$.pred)
# colnames(submission_1) = c('Id','SalePrice')
# write.csv(submission_1, 'pred_ver3.csv',row.names = FALSE)
```


# Conclution

Based on the estimate of RMSE from all the model, I decided to use prediction from liear regression. Here is my result on kaggle. I did not do good job, but my RMSE itself was considered as a good result because there are so many people participated in the competition and their scores are very cloase to each other.  

![Kaggle](/Users/shuseiyokoi/Desktop/Screen Shot 2021-06-03 at 19.11.03 (2).png)